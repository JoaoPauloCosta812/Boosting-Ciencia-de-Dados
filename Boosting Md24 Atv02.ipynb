{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b4340a-5d89-43e3-bf27-d48215347373",
   "metadata": {},
   "source": [
    "# Modulo 24 Atividade 02\n",
    "#### João Paulo Costa\n",
    "\n",
    "## AdaBoost e GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50634116-6408-4d52-a36f-db57b7c2bc95",
   "metadata": {},
   "source": [
    "#### 1. Cinco diferenças entre o AdaBoost e o GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dac3d5-5102-4aa6-b986-1b619cabf3de",
   "metadata": {},
   "source": [
    "| Aspecto                             | **AdaBoost**                                                         | **GBM (Gradient Boosting Machine)**                                                      |\n",
    "| ----------------------------------- | -------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **Função de perda (loss function)** | Usa basicamente o *exponencial loss* (mais voltado a classificação). | Pode usar várias funções de perda (ex: deviance, MSE, MAE), o que o torna mais flexível. |\n",
    "| **Atualização dos pesos**           | Ajusta pesos das amostras mal classificadas a cada iteração.         | Ajusta o modelo pela direção do gradiente negativo do erro da função de perda.           |\n",
    "| **Modelo base (weak learner)**      | Tipicamente usa *árvores rasas (stumps)*.                            | Usa árvores mais profundas, podendo ajustar `max_depth` para maior complexidade.         |\n",
    "| **Sensibilidade a ruído**           | Muito sensível a outliers, pois aumenta pesos de erros grandes.      | Mais robusto, pois a otimização via gradiente tende a suavizar erros extremos.           |\n",
    "| **Otimização**                      | Baseada em reponderação das amostras.                                | Baseada em *gradiente descendente* no espaço dos modelos.                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8703be-b6d1-4f68-a00e-89dc104d87c7",
   "metadata": {},
   "source": [
    "#### 2. Exemplo de classificação e regressão com GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02a1201-8e6a-4ee6-b511-d5c6187f640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 1.0\n",
      "\n",
      "Relatório de classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Carregar dados\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Criar e treinar modelo\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    ")\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# 3. Prever e avaliar\n",
    "y_pred = gbm.predict(X_test)\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nRelatório de classificação:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d20d19-59c9-4c19-9953-b07a7b4dc4f2",
   "metadata": {},
   "source": [
    "#### 3. Cinco hiperparâmetros importantes do GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee9309-bc25-4b23-adf9-6f0ca81554b5",
   "metadata": {},
   "source": [
    "| Hiperparâmetro  | Descrição                                                                          |\n",
    "| --------------- | ---------------------------------------------------------------------------------- |\n",
    "| `n_estimators`  | Número de árvores a serem criadas (iterações de boosting).                         |\n",
    "| `learning_rate` | Taxa de aprendizado (shrinkage) que controla o peso de cada árvore.                |\n",
    "| `max_depth`     | Profundidade máxima das árvores (controla complexidade e overfitting).             |\n",
    "| `subsample`     | Proporção dos dados usados em cada iteração (para Stochastic GBM).                 |\n",
    "| `loss`          | Função de perda a ser minimizada (ex: “deviance”, “exponential”, “squared_error”). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81423c72-b515-4c8b-82cd-274e8870f888",
   "metadata": {},
   "source": [
    "#### 4. GridSearchCV para otimização dos hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0589e3-b441-44fe-b67a-68e7bea95fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Melhor acurácia: 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grade de parâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    gbm, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "print(\"Melhor acurácia:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9ea54-7f61-45e8-83e1-99c07f0a760b",
   "metadata": {},
   "source": [
    "#### 5. Diferença entre GBM e Stochastic GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713fd48-061f-475d-aa12-4a7ba9fd2f1f",
   "metadata": {},
   "source": [
    "| Característica           | GBM tradicional                                         | Stochastic GBM                                                                |\n",
    "| ------------------------ | ------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **Amostragem dos dados** | Usa todo o conjunto de dados em cada iteração.          | Usa uma **amostra aleatória (subsample < 1)** em cada iteração.               |\n",
    "| **Objetivo**             | Minimizar erro global com base em todas as observações. | Introduzir *aleatoriedade* para reduzir overfitting e melhorar generalização. |\n",
    "| **Vantagem**             | Pode sobreajustar se `n_estimators` for alto.           | Melhora desempenho em dados ruidosos e reduz variância.                       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
