{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628f3e8c-0cfe-4f03-8b31-a2717d8bfbc9",
   "metadata": {},
   "source": [
    "# Modulo 24 Atividade 01\n",
    "#### João Paulo Costa\n",
    "\n",
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b6244-936a-4be6-909a-7662d0ffab07",
   "metadata": {},
   "source": [
    "#### 1. Cinco diferenças entre Random Forest e AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa426f2-4485-478d-b8e4-5c5be8b78ffa",
   "metadata": {},
   "source": [
    "| Aspecto                              | Random Forest                                                                                     | AdaBoost                                                                                 |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| **Tipo de Ensemble**                 | *Bagging* (Bootstrap Aggregating)                                                                 | *Boosting*                                                                               |\n",
    "| **Construção dos modelos**           | Constrói várias árvores independentes, cada uma com amostras aleatórias dos dados e das features. | Constrói modelos sequenciais, onde cada novo modelo tenta corrigir os erros do anterior. |\n",
    "| **Peso das observações**             | Cada amostra tem o mesmo peso no treinamento.                                                     | As amostras mal classificadas recebem peso maior em iterações seguintes.                 |\n",
    "| **Combinação final**                 | Média (para regressão) ou voto da maioria (para classificação).                                   | Combinação ponderada dos modelos, onde os mais precisos têm maior peso.                  |\n",
    "| **Sensibilidade a ruído e outliers** | Mais robusto a ruído e outliers.                                                                  | Mais sensível a ruído e outliers, pois aumenta o peso dos erros.                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaac4f2-f882-4da6-9e33-b2969eb121b7",
   "metadata": {},
   "source": [
    "#### 2. Exemplo prático do AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2b389d-04cb-4890-a250-8038546b7dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 1.0\n",
      "\n",
      "Relatório de classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Carregar base Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# 2. Dividir treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Criar modelo AdaBoost\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# 4. Treinar modelo\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# 5. Fazer previsões\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "# 6. Avaliar\n",
    "print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nRelatório de classificação:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f034e-3b1a-49b6-ab35-c8e8bcc087c1",
   "metadata": {},
   "source": [
    "#### 3. Cinco hiperparâmetros importantes no AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82a8dc-3256-4d32-ac82-2ee56978f9c0",
   "metadata": {},
   "source": [
    "| Hiperparâmetro   | Descrição                                                                                            |\n",
    "| ---------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| `n_estimators`   | Número de estimadores (modelos fracos) sequenciais a serem treinados.                                |\n",
    "| `learning_rate`  | Controla o peso dado a cada estimador. Valores menores reduzem overfitting, mas exigem mais árvores. |\n",
    "| `base_estimator` | Modelo fraco base (por padrão, `DecisionTreeClassifier(max_depth=1)`).                               |\n",
    "| `random_state`   | Garante reprodutibilidade.                                                                           |\n",
    "| `algorithm`      | Pode ser `\"SAMME\"` (para múltiplas classes) ou `\"SAMME.R\"` (padrão e mais eficiente).                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd869899-ed8a-4ed6-96a4-9e9f110030af",
   "metadata": {},
   "source": [
    "#### 4. GridSearchCV para encontrar melhores hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31523bfc-6e79-4959-9a31-c616da6ca631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Melhor acurácia: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir grade de parâmetros\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Criar modelo base\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(ada, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "print(\"Melhor acurácia:\", grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
